{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cbcab-2cad-40e5-a5a6-11094c60c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Standard Library ───────────────────────────────────────────────────────────\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import cycle\n",
    "\n",
    "# ── Scientific & Data Handling ────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "\n",
    "# ── PyTorch & Training Utilities ──────────────────────────────────────────────\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── Computer Vision & Augmentations ───────────────────────────────────────────\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pydicom import dcmread\n",
    "\n",
    "# ── Scikit-Learn & Statistics ────────────────────────────────────────────────\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, auc, roc_curve, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from scipy import interp\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "\n",
    "# ── Visualization ─────────────────────────────────────────────────────────────\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ── Grad-CAM & Explainability ────────────────────────────────────────────────\n",
    "from pytorch_grad_cam import (\n",
    "    GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n",
    "    AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.feature_factorization.deep_feature_factorization import \\\n",
    "    DeepFeatureFactorization\n",
    "import pytorch_grad_cam.utils.model_targets\n",
    "import pytorch_grad_cam.utils.reshape_transforms\n",
    "import pytorch_grad_cam.metrics.cam_mult_image\n",
    "import pytorch_grad_cam.metrics.road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadd459-ef0a-4a55-9de0-c57eecc5349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"Dataset Directory\")\n",
    "visualization_dir = \"visualization_dir\"\n",
    "model_save_dir = \"Directory_Model\"\n",
    "df = pd.read_csv(\"PLOSONE_DF-Stages\") \n",
    "df[\"image_id\"] = np.arange(len(df))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8343411",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5']\n",
    "class_labels = { name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 20\n",
    "SIZE =   496\n",
    "num_classes = class_labels\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354df16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.ShiftScaleRotate(rotate_limit=1.0, p=0.8, border_mode = 0),\n",
    "    A.Resize(height=SIZE, width=SIZE, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "])\n",
    "\n",
    "transforms_valid = A.Compose([\n",
    "    A.Resize(height=SIZE, width=SIZE, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "])\n",
    "\n",
    "transforms_test = A.Compose([\n",
    "    A.Resize(height=SIZE, width=SIZE, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f63273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctModel(nn.Module): \n",
    "    def __init__(self, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('resnet18', pretrained=True, in_chans=3) \n",
    "        self.logit = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.logit(x)\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = OctModel(num_classes=6)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['Path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        labels = self.df.iloc[idx][class_names].values.astype(np.float32)\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "        sample_id = self.df.iloc[idx]['image_id']\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=np.array(image))\n",
    "            image = transformed['image']\n",
    "        else:\n",
    "            image = np.array(image, dtype=np.float32) / 255.0  # Normalize the image to [0, 1]\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # Move the channel dimension\n",
    "\n",
    "        return image, labels, sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradcam_and_overlay(image, model_output, cam):\n",
    "    # Ensure model_output is 2D: [batch_size, num_classes]\n",
    "    if model_output.dim() == 1:\n",
    "        model_output = model_output.unsqueeze(0)\n",
    "    \n",
    "    # Determine the predicted class\n",
    "    _, predicted_class = model_output.max(dim=1)\n",
    "    \n",
    "    heatmap = cam.generate_heatmap(image, predicted_class)\n",
    "    \n",
    "    # Normalize the heatmap for visualization\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert torch tensor image to numpy for visualization\n",
    "    image_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    image_np = (image_np - np.min(image_np)) / (np.max(image_np) - np.min(image_np)) * 255\n",
    "    image_np = image_np.astype(np.uint8)\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    overlayed_img = cv2.addWeighted(image_np, 0.5, heatmap, 0.5, 0)\n",
    "    \n",
    "    return overlayed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(v, t, model, criterion, optimizer, dataloader_train, dataloader_valid, model_save_dir, patience=10):\n",
    "    val_fold_results = []\n",
    "    best_fold_ap = 0 \n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print(f'  Epoch {epoch + 1}/{N_EPOCHS}')\n",
    "\n",
    "        # ---- TRAINING PHASE ----\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "\n",
    "        for _, batch in enumerate(dataloader_train):\n",
    "            images, labels = batch[0].to(device).float(), batch[1].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.squeeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        # ---- VALIDATION PHASE ----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        for _, batch in enumerate(dataloader_valid):\n",
    "            images, labels = batch[0].to(device).float(), batch[1].to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels.squeeze(-1))\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.detach().cpu().numpy())\n",
    "                val_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        # Compute Average Precision Score (Macro)\n",
    "        val_preds = np.array(val_preds)\n",
    "        val_labels = np.array(val_labels)\n",
    "        ap_score_current = average_precision_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "        print(f\"AP Score Current Validation (Macro): {ap_score_current}\")\n",
    "\n",
    "        # Save the best model based on AP Score\n",
    "        if ap_score_current > best_fold_ap:\n",
    "            best_fold_ap = ap_score_current\n",
    "            epochs_no_improve = 0\n",
    "            best_model_path = os.path.join(model_save_dir, f\"Model{v}_T{t}Stage.pth\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Updated Best Model Saved at: {best_model_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered at epoch: {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        val_fold_results.append({\n",
    "            'TEST': t,\n",
    "            'VAL': v,\n",
    "            'epoch': epoch,\n",
    "            'train_loss': tr_loss / len(dataloader_train),\n",
    "            'valid_loss': val_loss / len(dataloader_valid),\n",
    "            'valid_ap_score': ap_score_current\n",
    "        })\n",
    "\n",
    "    return val_fold_results, best_fold_ap, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Score Current Validation (Macro): 0.46544990044752277\n",
      "Early stopping triggered at epoch: 28\n",
      "Fold completed with ROC-AUC: 0.9442127786265396\n",
      "  Epoch 1/30\n",
      "AP Score Current Validation (Macro): 0.40194245081023056\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 2/30\n",
      "AP Score Current Validation (Macro): 0.41446235073693516\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 3/30\n",
      "AP Score Current Validation (Macro): 0.41502886817967033\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 4/30\n",
      "AP Score Current Validation (Macro): 0.442364623680959\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 5/30\n",
      "AP Score Current Validation (Macro): 0.4493133397196188\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 6/30\n",
      "AP Score Current Validation (Macro): 0.4287099238604337\n",
      "  Epoch 7/30\n",
      "AP Score Current Validation (Macro): 0.3919082106580569\n",
      "  Epoch 8/30\n",
      "AP Score Current Validation (Macro): 0.4544891980735321\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 9/30\n",
      "AP Score Current Validation (Macro): 0.43249804663780766\n",
      "  Epoch 10/30\n",
      "AP Score Current Validation (Macro): 0.4241460127807941\n",
      "  Epoch 11/30\n",
      "AP Score Current Validation (Macro): 0.4563184279232046\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 12/30\n",
      "AP Score Current Validation (Macro): 0.4886140599722282\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 13/30\n",
      "AP Score Current Validation (Macro): 0.4684914737721167\n",
      "  Epoch 14/30\n",
      "AP Score Current Validation (Macro): 0.4566889235771341\n",
      "  Epoch 15/30\n",
      "AP Score Current Validation (Macro): 0.4380818070266346\n",
      "  Epoch 16/30\n",
      "AP Score Current Validation (Macro): 0.455751104244696\n",
      "  Epoch 17/30\n",
      "AP Score Current Validation (Macro): 0.43087888888773845\n",
      "  Epoch 18/30\n",
      "AP Score Current Validation (Macro): 0.50796142394469\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 19/30\n",
      "AP Score Current Validation (Macro): 0.4914500636578412\n",
      "  Epoch 20/30\n",
      "AP Score Current Validation (Macro): 0.5058836111877987\n",
      "  Epoch 21/30\n",
      "AP Score Current Validation (Macro): 0.4500392427629554\n",
      "  Epoch 22/30\n",
      "AP Score Current Validation (Macro): 0.45453878797049674\n",
      "  Epoch 23/30\n",
      "AP Score Current Validation (Macro): 0.45131965300140225\n",
      "  Epoch 24/30\n",
      "AP Score Current Validation (Macro): 0.45273874921288004\n",
      "  Epoch 25/30\n",
      "AP Score Current Validation (Macro): 0.45784201839712835\n",
      "  Epoch 26/30\n",
      "AP Score Current Validation (Macro): 0.48853628065870236\n",
      "  Epoch 27/30\n",
      "AP Score Current Validation (Macro): 0.4087885978222205\n",
      "  Epoch 28/30\n",
      "AP Score Current Validation (Macro): 0.5166740747933617\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T0Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 29/30\n",
      "AP Score Current Validation (Macro): 0.502018901053442\n",
      "  Epoch 30/30\n",
      "AP Score Current Validation (Macro): 0.44349262607655854\n",
      "Fold completed with ROC-AUC: 0.9342539526436365\n",
      "  Epoch 1/30\n",
      "AP Score Current Validation (Macro): 0.43946368253927864\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T1Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 2/30\n",
      "AP Score Current Validation (Macro): 0.4376223927905227\n",
      "  Epoch 3/30\n",
      "AP Score Current Validation (Macro): 0.46108574106405414\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T1Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 4/30\n",
      "AP Score Current Validation (Macro): 0.43431348057586067\n",
      "  Epoch 5/30\n",
      "AP Score Current Validation (Macro): 0.45365018660115086\n",
      "  Epoch 6/30\n",
      "AP Score Current Validation (Macro): 0.4121176113185339\n",
      "  Epoch 7/30\n",
      "AP Score Current Validation (Macro): 0.4614872977760634\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T1Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 8/30\n",
      "AP Score Current Validation (Macro): 0.4488998680557801\n",
      "  Epoch 9/30\n",
      "AP Score Current Validation (Macro): 0.5117951039289976\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T1Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 10/30\n",
      "AP Score Current Validation (Macro): 0.5078709806276903\n",
      "  Epoch 11/30\n",
      "AP Score Current Validation (Macro): 0.49488701434792315\n",
      "  Epoch 12/30\n",
      "AP Score Current Validation (Macro): 0.48617463541805345\n",
      "  Epoch 13/30\n",
      "AP Score Current Validation (Macro): 0.4645064927372004\n",
      "  Epoch 14/30\n",
      "AP Score Current Validation (Macro): 0.4731251861256922\n",
      "  Epoch 15/30\n",
      "AP Score Current Validation (Macro): 0.5213132091591831\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T1Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 16/30\n",
      "AP Score Current Validation (Macro): 0.44422023138930183\n",
      "  Epoch 17/30\n",
      "AP Score Current Validation (Macro): 0.47702148201336986\n",
      "  Epoch 18/30\n",
      "AP Score Current Validation (Macro): 0.4384095086061574\n",
      "  Epoch 19/30\n",
      "AP Score Current Validation (Macro): 0.5081760980716559\n",
      "  Epoch 20/30\n",
      "AP Score Current Validation (Macro): 0.3879397189489538\n",
      "  Epoch 21/30\n",
      "AP Score Current Validation (Macro): 0.47735117441785174\n",
      "  Epoch 22/30\n",
      "AP Score Current Validation (Macro): 0.4690646617162173\n",
      "  Epoch 23/30\n",
      "AP Score Current Validation (Macro): 0.43749347376423975\n",
      "  Epoch 24/30\n",
      "AP Score Current Validation (Macro): 0.45315128424154344\n",
      "  Epoch 25/30\n",
      "AP Score Current Validation (Macro): 0.49711587595048745\n",
      "Early stopping triggered at epoch: 25\n",
      "Fold completed with ROC-AUC: 0.9362747205451584\n",
      "  Epoch 1/30\n",
      "AP Score Current Validation (Macro): 0.36839374672813346\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T2Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 2/30\n",
      "AP Score Current Validation (Macro): 0.43754978250492993\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T2Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 3/30\n",
      "AP Score Current Validation (Macro): 0.4129709961957642\n",
      "  Epoch 4/30\n",
      "AP Score Current Validation (Macro): 0.4346742618950718\n",
      "  Epoch 5/30\n",
      "AP Score Current Validation (Macro): 0.4495980599333118\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T2Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 6/30\n",
      "AP Score Current Validation (Macro): 0.43446358740540064\n",
      "  Epoch 7/30\n",
      "AP Score Current Validation (Macro): 0.3924799072313538\n",
      "  Epoch 8/30\n",
      "AP Score Current Validation (Macro): 0.4540763693405001\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T2Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 9/30\n",
      "AP Score Current Validation (Macro): 0.37380979055240476\n",
      "  Epoch 10/30\n",
      "AP Score Current Validation (Macro): 0.5151873535622981\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T2Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 11/30\n",
      "AP Score Current Validation (Macro): 0.4486294799176555\n",
      "  Epoch 12/30\n",
      "AP Score Current Validation (Macro): 0.42270762106298815\n",
      "  Epoch 13/30\n",
      "AP Score Current Validation (Macro): 0.49632522012657937\n",
      "  Epoch 14/30\n",
      "AP Score Current Validation (Macro): 0.4828125960318122\n",
      "  Epoch 15/30\n",
      "AP Score Current Validation (Macro): 0.48583885921735454\n",
      "  Epoch 16/30\n",
      "AP Score Current Validation (Macro): 0.47161535551143624\n",
      "  Epoch 17/30\n",
      "AP Score Current Validation (Macro): 0.49491194768368557\n",
      "  Epoch 18/30\n",
      "AP Score Current Validation (Macro): 0.4808794398087384\n",
      "  Epoch 19/30\n",
      "AP Score Current Validation (Macro): 0.4493713685298249\n",
      "  Epoch 20/30\n",
      "AP Score Current Validation (Macro): 0.4623375580976505\n",
      "Early stopping triggered at epoch: 20\n",
      "Fold completed with ROC-AUC: 0.9258379593145712\n",
      "  Epoch 1/30\n",
      "AP Score Current Validation (Macro): 0.4103268762237607\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 2/30\n",
      "AP Score Current Validation (Macro): 0.39777529544992846\n",
      "  Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Score Current Validation (Macro): 0.44227589526618444\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 4/30\n",
      "AP Score Current Validation (Macro): 0.44971257459231634\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 5/30\n",
      "AP Score Current Validation (Macro): 0.46537312874450404\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 6/30\n",
      "AP Score Current Validation (Macro): 0.4255749354209137\n",
      "  Epoch 7/30\n",
      "AP Score Current Validation (Macro): 0.4469133951894683\n",
      "  Epoch 8/30\n",
      "AP Score Current Validation (Macro): 0.4396246028477239\n",
      "  Epoch 9/30\n",
      "AP Score Current Validation (Macro): 0.4451012712945613\n",
      "  Epoch 10/30\n",
      "AP Score Current Validation (Macro): 0.47167884483108796\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 11/30\n",
      "AP Score Current Validation (Macro): 0.45912171933214685\n",
      "  Epoch 12/30\n",
      "AP Score Current Validation (Macro): 0.4414286806254295\n",
      "  Epoch 13/30\n",
      "AP Score Current Validation (Macro): 0.47192576102002787\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 14/30\n",
      "AP Score Current Validation (Macro): 0.4511054383348088\n",
      "  Epoch 15/30\n",
      "AP Score Current Validation (Macro): 0.4724220225610128\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 16/30\n",
      "AP Score Current Validation (Macro): 0.456582327750026\n",
      "  Epoch 17/30\n",
      "AP Score Current Validation (Macro): 0.41396054320846076\n",
      "  Epoch 18/30\n",
      "AP Score Current Validation (Macro): 0.4517197615325652\n",
      "  Epoch 19/30\n",
      "AP Score Current Validation (Macro): 0.44218048947969346\n",
      "  Epoch 20/30\n",
      "AP Score Current Validation (Macro): 0.4471353366789362\n",
      "  Epoch 21/30\n",
      "AP Score Current Validation (Macro): 0.47492687778352716\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 22/30\n",
      "AP Score Current Validation (Macro): 0.45953470965828597\n",
      "  Epoch 23/30\n",
      "AP Score Current Validation (Macro): 0.4514259337271495\n",
      "  Epoch 24/30\n",
      "AP Score Current Validation (Macro): 0.48222432039662827\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 25/30\n",
      "AP Score Current Validation (Macro): 0.45491850292325897\n",
      "  Epoch 26/30\n",
      "AP Score Current Validation (Macro): 0.5047770464472768\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "  Epoch 27/30\n",
      "AP Score Current Validation (Macro): 0.49299415115929524\n",
      "  Epoch 28/30\n",
      "AP Score Current Validation (Macro): 0.4691180350582161\n",
      "  Epoch 29/30\n",
      "AP Score Current Validation (Macro): 0.3810487966977834\n",
      "  Epoch 30/30\n",
      "AP Score Current Validation (Macro): 0.5246250754356766\n",
      "Updated Best Model Saved at: D:/weights/SKEVAS/REG/SKEVAS_REG_GRAD_OCT-GRAD_fold-V4_T3Stages6-UE100e-PAPERFINALEE.pth\n",
      "Fold completed with ROC-AUC: 0.9355860896035431\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(visualization_dir, exist_ok=True)\n",
    "model = OctModel(num_classes=6).to(device)\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=[model.model.layer4[-1].conv2])\n",
    "\n",
    "aggregated_results = []\n",
    "for test_fold in range(5):\n",
    "    for val_fold in range(5):\n",
    "        if test_fold != val_fold:\n",
    "            aggregated_test_targets = []\n",
    "            aggregated_test_probabilities = []\n",
    "            aggregated_test_ids = []  # To store the IDs of the test images\n",
    "\n",
    "            model.apply(reset_weights)  # Reset model weights\n",
    "\n",
    "            # Prepare dataset splits\n",
    "            val_df = df[df['fold'] == val_fold].reset_index(drop=True)\n",
    "            train_df = df[(df['fold'] != test_fold) & (df['fold'] != val_fold)].reset_index(drop=True)\n",
    "            test_df = df[df['fold'] == test_fold].reset_index(drop=True)\n",
    "\n",
    "            # Prepare dataloaders\n",
    "            dataset_train = OctDataset(df=train_df, transforms=transforms_train)\n",
    "            dataset_valid = OctDataset(df=val_df, transforms=transforms_valid)\n",
    "            dataset_test = OctDataset(df=test_df, transforms=transforms_test)\n",
    "\n",
    "            dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "            dataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "            dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "            # Training setup\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "            # Train and validate\n",
    "            best_model_path = os.path.join(model_save_dir, f\"Model{test_fold}_T{val_fold}Stages.pth\")\n",
    "            val_fold_results, best_fold_roc_auc, best_model_path = train_one_fold(\n",
    "                test_fold, val_fold, model, criterion, optimizer, dataloader_train, dataloader_valid, model_save_dir, patience=10\n",
    "            )\n",
    "\n",
    "            fold_test_targets = []\n",
    "            fold_test_probabilities = []\n",
    "            fold_test_predictions = []  # Initialize predictions list\n",
    "            fold_test_ids = []  # To store test IDs for the fold\n",
    "\n",
    "            for batch_idx, batch in enumerate(dataloader_test):\n",
    "                images, labels, ids = batch[0].to(device).float(), batch[1].to(device).float(), batch[2]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    images = images.type(torch.cuda.FloatTensor)\n",
    "                    outputs = model(images)\n",
    "                    probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                fold_test_targets.extend(labels.cpu().numpy())\n",
    "                fold_test_probabilities.extend(probabilities.tolist())\n",
    "                fold_test_ids.extend(ids.numpy())\n",
    "\n",
    "                for i, (img, output, label) in enumerate(zip(images, outputs, labels)):\n",
    "                    # Define true and predicted labels\n",
    "                    true_label_idx = label.argmax().item()\n",
    "                    pred_class_idx = output.argmax().item()\n",
    "\n",
    "                    # Prepare the input image and class index for Grad-CAM\n",
    "                    input_tensor = img.unsqueeze(0)\n",
    "                    target = ClassifierOutputTarget(pred_class_idx)\n",
    "\n",
    "                    # Generate the CAM mask\n",
    "                    cam_mask = cam(input_tensor=input_tensor, targets=[target])\n",
    "                    cam_mask = cam_mask[0, :]\n",
    "\n",
    "                    # Prepare the original image for visualization\n",
    "                    img_np = img.cpu().numpy().transpose(1, 2, 0)\n",
    "                    img_normalized = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "                    # Apply the CAM mask to the image\n",
    "                    visualization = show_cam_on_image(img_normalized, cam_mask, use_rgb=True)\n",
    "\n",
    "                    # Plot the original and Grad-CAM visualizations side by side\n",
    "                    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "                    # Original image\n",
    "                    axs[0].imshow(img_normalized, cmap='gray')\n",
    "                    axs[0].set_title(f\"Original Image\\nTrue: {true_label_idx}\")\n",
    "                    axs[0].axis('off')\n",
    "\n",
    "                    # Grad-CAM visualization\n",
    "                    axs[1].imshow(visualization)\n",
    "                    axs[1].set_title(f\"Predicted: {pred_class_idx}, True: {true_label_idx}\")\n",
    "                    axs[1].axis('off')\n",
    "\n",
    "                    image_filename = f\"image_{batch_idx * len(images) + i + 1}_true_{true_label_idx}_pred_{pred_class_idx}.png\"\n",
    "                    plt.savefig(os.path.join(visualization_dir, image_filename))\n",
    "                    plt.close(fig)\n",
    "\n",
    "            print(\"Fold completed with ROC-AUC:\", roc_auc_score(np.stack(fold_test_targets), np.stack(fold_test_probabilities)))\n",
    "\n",
    "            aggregated_test_targets.extend(fold_test_targets)\n",
    "            aggregated_test_probabilities.extend(fold_test_probabilities)\n",
    "            aggregated_test_ids.extend(fold_test_ids)\n",
    "\n",
    "            sub = pd.DataFrame(aggregated_test_probabilities, columns=[\"pred0\", \"pred1\", \"pred2\",\"pred3\",\"pred4\",\"pred5\"])\n",
    "            sub[\"image_id\"] = fold_test_ids\n",
    "            sub[\"test_fold\"] = test_fold\n",
    "            sub[\"val_fold\"] = val_fold\n",
    "            aggregated_results.append(sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
